# æ™ºèƒ½åŒ–RAGç³»ç»Ÿä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨åŸºäºLLMæ„å›¾åˆ†è§£çš„æ™ºèƒ½åŒ–3Dåœºæ™¯RAGç³»ç»Ÿï¼Œå®ç°åŠ¨æ€æ•°æ®åº“å¼çš„é«˜æ•ˆæ£€ç´¢ã€‚

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
4. [APIå‚è€ƒ](#apiå‚è€ƒ)
5. [é…ç½®é€‰é¡¹](#é…ç½®é€‰é¡¹)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
7. [ç”Ÿäº§éƒ¨ç½²](#ç”Ÿäº§éƒ¨ç½²)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

æ™ºèƒ½RAGç³»ç»Ÿé€šè¿‡ä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯å®ç°é«˜æ•ˆçš„3Dåœºæ™¯æ£€ç´¢ï¼š

### æ ¸å¿ƒç‰¹æ€§
- **LLMé©±åŠ¨çš„æŸ¥è¯¢æ„å›¾åˆ†è§£**: å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–çš„æŸ¥è¯¢æ„å›¾
- **åŸºäºæ„å›¾çš„æ¨¡å‹å‰ªæ**: æ ¹æ®æŸ¥è¯¢æ„å›¾åŠ¨æ€è¿‡æ»¤æ— å…³çš„é«˜æ–¯ç‚¹ï¼Œå‡å°‘è®¡ç®—é‡
- **å¤šå› å­é‡æ’åº**: ç»¼åˆå‘é‡ç›¸ä¼¼åº¦ã€æ–‡æœ¬ç›¸ä¼¼åº¦ã€è§†è§‰ç›¸ä¼¼åº¦ã€ç©ºé—´ç›¸å…³æ€§å’Œå¤šè§†è§’ä¸€è‡´æ€§
- **ç¼“å­˜ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜æ„å›¾å’Œç»“æœï¼Œæå‡å“åº”é€Ÿåº¦
- **åŠ¨æ€æ•°æ®åº“**: ç±»ä¼¼ä¼ ç»Ÿæ•°æ®åº“çš„æŸ¥è¯¢ä¼˜åŒ–ï¼Œä½†é’ˆå¯¹3Dåœºæ™¯æ•°æ®

### æŠ€æœ¯æ¶æ„
```
è‡ªç„¶è¯­è¨€æŸ¥è¯¢ â†’ LLMæ„å›¾åˆ†æ â†’ é«˜æ–¯ç‚¹å‰ªæ â†’ åˆå§‹æ£€ç´¢ â†’ å¤šå› å­é‡æ’åº â†’ æœ€ç»ˆç»“æœ
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install torch torchvision numpy scipy scikit-learn faiss-cpu
pip install openai aiohttp  # OpenAI-compatible LLM API
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from intelligent_rag import create_intelligent_rag, quick_search

# æ–¹å¼1: ä½¿ç”¨ä¾¿æ·å‡½æ•°
async def simple_search():
    results = await quick_search(
        query="çº¢è‰²çš„ç°ä»£æ¤…å­",
        model_path="./model/scene.ply",
        api_key="your-api-key",
        top_k=5
    )
    print(f"æ‰¾åˆ° {len(results['final_results'])} ä¸ªç»“æœ")

# æ–¹å¼2: åˆ›å»ºæŒä¹…åŒ–ç³»ç»Ÿ
async def persistent_system():
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = create_intelligent_rag(
        model_path="./model/scene.ply",
        api_key="your-api-key"
    )
    
    # æ‰§è¡Œå¤šæ¬¡æœç´¢
    queries = ["çº¢è‰²æ¤…å­", "å®¢å…æ¡Œå­", "ç°ä»£è£…é¥°"]
    for query in queries:
        results = await rag.intelligent_search(query, top_k=5)
        print(f"æŸ¥è¯¢: {query} - ç»“æœ: {len(results['final_results'])}")

# è¿è¡Œç¤ºä¾‹
asyncio.run(simple_search())
```

### 3. é…ç½®LLMæä¾›å•†

```python
# ä½¿ç”¨OpenAI GPT
rag = create_intelligent_rag(
    model_path="./model/scene.ply",
    api_key="your-openai-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-4"
)

# ä½¿ç”¨Azure OpenAI
rag = create_intelligent_rag(
    model_path="./model/scene.ply",
    api_key="your-azure-api-key",
    base_url="https://your-resource.openai.azure.com/openai/deployments/your-deployment/",
    model="gpt-4"
)

# ä½¿ç”¨vLLMæœ¬åœ°éƒ¨ç½²
rag = create_intelligent_rag(
    model_path="./model/scene.ply",
    api_key="dummy",
    base_url="http://localhost:8000/v1",
    model="mistral-7b-instruct"
)
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. æŸ¥è¯¢æ„å›¾åˆ†æ

ç³»ç»Ÿè‡ªåŠ¨å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–æ„å›¾ï¼š

```python
# æŸ¥è¯¢æ„å›¾åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
{
    "query_type": "object_search",           # æŸ¥è¯¢ç±»å‹
    "primary_objects": ["chair"],            # ä¸»è¦å¯¹è±¡
    "secondary_objects": ["furniture"],      # æ¬¡è¦å¯¹è±¡
    "spatial_constraints": {                 # ç©ºé—´çº¦æŸ
        "location": "center",
        "bounds": {"x": [-2, 2], "y": [0, 1], "z": [-2, 2]}
    },
    "visual_attributes": {                   # è§†è§‰å±æ€§
        "color": ["red"],
        "material": ["wood"],
        "style": ["modern"]
    },
    "semantic_context": {                    # è¯­ä¹‰ä¸Šä¸‹æ–‡
        "scene_type": "living_room",
        "function": "seating"
    },
    "confidence": 0.95,                      # åˆ†æç½®ä¿¡åº¦
    "priority_weights": {                    # å› å­æƒé‡
        "vector_similarity": 0.3,
        "text_similarity": 0.2,
        "visual_similarity": 0.2,
        "spatial_relevance": 0.2,
        "multi_view_consistency": 0.1
    }
}
```

### 2. æ¨¡å‹å‰ªæ

åŸºäºæŸ¥è¯¢æ„å›¾åŠ¨æ€å‡å°‘éœ€è¦å¤„ç†çš„é«˜æ–¯ç‚¹ï¼š

```python
# å‰ªææ•ˆæœç¤ºä¾‹
åŸå§‹æ¨¡å‹: 10,000 ä¸ªé«˜æ–¯ç‚¹
å‰ªæå:   3,000 ä¸ªé«˜æ–¯ç‚¹ (70% å‡å°‘)
ç†è®ºåŠ é€Ÿ: 3.33x
å®é™…åŠ é€Ÿ: 2.5x (è€ƒè™‘å¼€é”€)
```

### 3. å¤šå› å­é‡æ’åº

ç»¼åˆå¤šä¸ªç›¸ä¼¼åº¦å› å­è¿›è¡Œç»“æœæ’åºï¼š

- **å‘é‡ç›¸ä¼¼åº¦**: CLIPç‰¹å¾å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦
- **æ–‡æœ¬ç›¸ä¼¼åº¦**: è¯­ä¹‰æ ‡ç­¾æ–‡æœ¬åŒ¹é…åº¦
- **è§†è§‰ç›¸ä¼¼åº¦**: é¢œè‰²ã€æè´¨ã€é£æ ¼åŒ¹é…åº¦
- **ç©ºé—´ç›¸å…³æ€§**: ä½ç½®ã€è¾¹ç•Œã€é‚»è¿‘æ€§è¯„åˆ†
- **å¤šè§†è§’ä¸€è‡´æ€§**: é«˜æ–¯ç‚¹å¯†åº¦å’Œç©ºé—´åˆ†å¸ƒ

## ğŸ“– APIå‚è€ƒ

### IntelligentRAG ç±»

#### åˆå§‹åŒ–
```python
rag = IntelligentRAG(
    model_path: str,           # 3DGSæ¨¡å‹è·¯å¾„
    llm_provider: LLMProvider, # LLMæä¾›å•†
    vector_db_path: str = None # å‘é‡æ•°æ®åº“è·¯å¾„
)
```

#### ä¸»è¦æ–¹æ³•

**intelligent_search**
```python
results = await rag.intelligent_search(
    query: str,                    # æœç´¢æŸ¥è¯¢
    top_k: int = 10,              # è¿”å›ç»“æœæ•°é‡
    downsample_factor: float = 0.3 # é™é‡‡æ ·å› å­
) -> Dict
```

**get_system_stats**
```python
stats = rag.get_system_stats()
# è¿”å›: {'model_gaussians': 10000, 'cached_intents': 5, ...}
```

**clear_cache**
```python
rag.clear_cache()  # æ¸…ç©ºç¼“å­˜
```

### ä¾¿æ·å‡½æ•°

**create_intelligent_rag**
```python
rag = create_intelligent_rag(
    model_path: str,
    api_key: str,
    base_url: str = "https://api.openai.com/v1",
    model: str = "gpt-4",
    **kwargs
)
```

**quick_search**
```python
results = await quick_search(
    query: str,
    model_path: str,
    api_key: str,
    base_url: str = "https://api.openai.com/v1",
    model: str = "gpt-4",
    top_k: int = 10
)
```

### ç»“æœæ ¼å¼

```python
{
    "success": True,
    "query": "çº¢è‰²æ¤…å­",
    "processing_time": 0.123,
    "final_results": [
        {
            "cluster_id": 0,
            "vector_similarity": 0.89,
            "text_similarity": 0.75,
            "visual_similarity": 0.92,
            "spatial_relevance": 0.67,
            "multi_view_consistency": 0.78,
            "final_score": 0.834
        }
    ],
    "intent": {...},
    "performance_metrics": {...}
}
```

## âš™ï¸ é…ç½®é€‰é¡¹

### é…ç½®æ–‡ä»¶ç¤ºä¾‹ (config.json)

```json
{
    "model_path": "./model/scene.ply",
    "llm_provider": {
        "type": "openai",
        "model": "gpt-4",
        "api_key": "your-api-key"
    },
    "vector_db_path": "./data/vectors.index",
    "cache_settings": {
        "max_intent_cache": 1000,
        "max_result_cache": 500,
        "cache_ttl": 3600
    },
    "search_settings": {
        "default_top_k": 10,
        "default_downsample": 0.3,
        "max_top_k": 50
    },
    "performance": {
        "enable_metrics": true,
        "log_slow_queries": true,
        "slow_query_threshold": 2.0
    }
}
```

### ç¯å¢ƒå˜é‡

```bash
export OPENAI_API_KEY="your-openai-key"
export RAG_MODEL_PATH="./model/scene.ply"
export RAG_CACHE_DIR="./cache"
export RAG_LOG_LEVEL="INFO"
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### 1. é™é‡‡æ ·å› å­è°ƒä¼˜

```python
# é€Ÿåº¦ vs è´¨é‡æƒè¡¡
downsample_factor = 0.1  # æœ€å¿«ï¼Œè´¨é‡è¾ƒä½
downsample_factor = 0.3  # å¹³è¡¡ï¼ˆæ¨èï¼‰
downsample_factor = 0.5  # è¾ƒæ…¢ï¼Œè´¨é‡è¾ƒå¥½
downsample_factor = 0.8  # æœ€æ…¢ï¼Œè´¨é‡æœ€é«˜
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# é¢„çƒ­å¸¸ç”¨æŸ¥è¯¢
warmup_queries = ["æ¤…å­", "æ¡Œå­", "æ²™å‘", "è£…é¥°å“"]
for query in warmup_queries:
    await rag.intelligent_search(query)
```

### 3. æ‰¹é‡å¤„ç†

```python
# å¯¹äºå¤§é‡æŸ¥è¯¢ï¼Œä½¿ç”¨æ‰¹é‡API
from production_usage import SceneRAGApplication

app = SceneRAGApplication()
await app.initialize()

results = await app.batch_search([
    "çº¢è‰²æ¤…å­",
    "ç°ä»£æ¡Œå­", 
    "èˆ’é€‚æ²™å‘"
])
```

### 4. å†…å­˜ä¼˜åŒ–

```python
# å®šæœŸæ¸…ç†ç¼“å­˜
if len(rag.intent_cache) > 1000:
    rag.clear_cache()

# ä½¿ç”¨å‘é‡æ•°æ®åº“æŒä¹…åŒ–
rag = create_intelligent_rag(
    model_path="./model/scene.ply",
    vector_db_path="./vectors.index"  # æŒä¹…åŒ–å‘é‡ç´¢å¼•
)
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²

### 1. Dockeréƒ¨ç½²

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "production_server.py"]
```

### 2. è´Ÿè½½å‡è¡¡é…ç½®

```python
# å¤šå®ä¾‹éƒ¨ç½²
instances = [
    create_intelligent_rag(model_path="./model1.ply"),
    create_intelligent_rag(model_path="./model2.ply"),
    create_intelligent_rag(model_path="./model3.ply")
]

# ç®€å•è½®è¯¢è´Ÿè½½å‡è¡¡
current_instance = 0

async def balanced_search(query):
    global current_instance
    instance = instances[current_instance]
    current_instance = (current_instance + 1) % len(instances)
    return await instance.intelligent_search(query)
```

### 3. ç›‘æ§å’Œæ—¥å¿—

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_system.log'),
        logging.StreamHandler()
    ]
)

# æ€§èƒ½ç›‘æ§
def monitor_performance(results):
    if results['processing_time'] > 2.0:
        logger.warning(f"æ…¢æŸ¥è¯¢: {results['query']} ({results['processing_time']:.3f}s)")
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. å†…å­˜ä¸è¶³**
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘é™é‡‡æ ·å› å­
downsample_factor = 0.1  # å‡å°‘å¤„ç†çš„é«˜æ–¯ç‚¹æ•°é‡
```

**2. APIè°ƒç”¨å¤±è´¥**
```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°æä¾›å•†ä½œä¸ºå›é€€
try:
    rag = create_intelligent_rag(provider_type="openai", api_key=api_key)
except:
    rag = create_intelligent_rag(provider_type="local")  # å›é€€åˆ°æœ¬åœ°
```

**3. æœç´¢ç»“æœè´¨é‡å·®**
```python
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´æƒé‡é…ç½®
intent.priority_weights = {
    "vector_similarity": 0.4,    # å¢åŠ å‘é‡ç›¸ä¼¼åº¦æƒé‡
    "text_similarity": 0.3,
    "visual_similarity": 0.15,
    "spatial_relevance": 0.1,
    "multi_view_consistency": 0.05
}
```

**4. å“åº”æ—¶é—´æ…¢**
```python
# è§£å†³æ–¹æ¡ˆï¼šå¯ç”¨ç¼“å­˜å’Œé¢„çƒ­
await rag.intelligent_search("å¸¸ç”¨æŸ¥è¯¢")  # é¢„çƒ­
rag.clear_cache()  # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
```

### è°ƒè¯•æ¨¡å¼

```python
import logging
logging.getLogger('intelligent_rag').setLevel(logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
results = await rag.intelligent_search(query, debug=True)
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å…¸å‹æ€§èƒ½æŒ‡æ ‡

```
æ¨¡å‹è§„æ¨¡: 10,000 é«˜æ–¯ç‚¹
æŸ¥è¯¢ç±»å‹: æ··åˆæŸ¥è¯¢

ä¸ä½¿ç”¨å‰ªæ:
- å¤„ç†æ—¶é—´: 2.1s
- å†…å­˜ä½¿ç”¨: 1.2GB

ä½¿ç”¨æ™ºèƒ½å‰ªæ (30% é™é‡‡æ ·):
- å¤„ç†æ—¶é—´: 0.8s (2.6x åŠ é€Ÿ)
- å†…å­˜ä½¿ç”¨: 0.4GB (67% å‡å°‘)
- å‡†ç¡®ç‡: 92% (è½»å¾®ä¸‹é™)
```

### æ‰©å±•æ€§æµ‹è¯•

```
10K é«˜æ–¯ç‚¹: 0.8s
100K é«˜æ–¯ç‚¹: 2.3s  
1M é«˜æ–¯ç‚¹: 8.7s
10M é«˜æ–¯ç‚¹: 35s (éœ€è¦åˆ†å¸ƒå¼å¤„ç†)
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚éœ€æŠ€æœ¯æ”¯æŒæˆ–æŠ¥å‘Šé—®é¢˜ï¼Œè¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–æäº¤GitHub Issueã€‚

## ğŸ”„ ç‰ˆæœ¬æ›´æ–°

- v1.0: åŸºç¡€RAGåŠŸèƒ½
- v1.1: æ·»åŠ LLMæ„å›¾åˆ†æ 
- v1.2: å®ç°æ¨¡å‹å‰ªæ
- v1.3: å¤šå› å­é‡æ’åº
- v1.4: ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ LICENSE æ–‡ä»¶ã€‚